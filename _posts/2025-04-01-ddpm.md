---
layout: single
title: "Understanding Denoising Diffusion Probabilistic Models"
categories: academics 
tag: [AI, Diffusion] 
---

## Introduction 
As a part of a group study session at my college's artificial intelligence club [HYU HAI](https://github.com/HanyangTechAI), I came across the _Denoising Diffusion Probabilistic Models_ paper. We studied the paper together, and here's what I learned from it.
- Forward & Reverse Process
- How Neural Network is used in the reverse process

**Citation**

[1] J. Ho, A. Jain, and P. Abbeel, “Denoising Diffusion Probabilistic Models,” _arXiv:2006.11239 [cs, stat]_, Dec. 2020, Available: https://arxiv.org/abs/2006.11239

[2] Auto-encoding variational Bayes

![](/assets/img/ddpm.png)

## Forward Process
**Forward process is the process of adding noise to an image**. It is also called diffusion process. Let the original image be represented by $X_0$. The forward process is defined as

$
q(X_{1:T}|X_0) := \prod_{t=1}^{T} q(X_t|X_{t-1}), \quad q(X_t|X_{t-1}) := \mathcal{N}(X_t; \sqrt{1 - \beta_t} X_{t-1}, \beta_t I)
$

According to this definition, the calculation needs to be done $T$ times in order to reach the final state, $X_T$. This can be computationally expansive. In order to solve this problem, the paper cites _Auto-encoding variational Bayes_ paper, and reparameterizes the diffusion process into

$
q(X_t | X_0) = \mathcal{N}(X_t; \sqrt{\bar{\alpha}_t} X_0, (1 - \bar{\alpha}_t) I)
$

where 
- $\alpha_t = 1 - \beta_t$
- $\bar{\alpha}_t = \Pi_{s=1}^t\alpha_s$

Notice how **the diffusion process is now defined as a single gaussian distribution**, with a new parameter $\alpha$. This reparameterization allow us to **skip the markov chain** and directly reach the image with noise at step $t$, $X_t$.

Now, the paper further reparameterizes them in terms of $\tilde{\mu}_t$ and $\tilde{\beta}_t$. These parameters will later be used for direct comparison between the reverse process's values in the loss function. Now the forward process takes the final form of 

$
q(X_t | X_0) = \mathcal{N}(X_t; \tilde{\mu}_t (X_t, X_0), \tilde{\beta}_t I)
$

where

- $\tilde{\mu}_t(X_t, X_0) := \frac{\sqrt{\bar{\alpha}_t - 1} \beta_t}{1 - \bar{\alpha}_t} X_0 + \frac{\sqrt{\alpha_t (1 - \bar{\alpha}_{t-1})}}{1 - \bar{\alpha}_t} X_t$

- $\quad \tilde{\beta}_t := \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t$

Now don't freak out here. $\tilde{\mu}_t$ is basically the answer for the diffusion process's posterior's expacted value. It is the value that we are aiming to predict using our neural network.

## Reverse Process
Now, the goal of this paper is to denoise that $X_T$ image back into $X_0$. In order to do that, a reverse process is defined as follow.

$
p_\theta(X_{0:T}) := p(X_T) \prod_{t=1}^{T} p_\theta(X_{t-1}|X_t), \quad p_\theta(X_{t-1}|X_t) := \mathcal{N}(X_{t-1}; \mu_\theta(X_t, t),\sum_\theta(X_t, t))
$

Notice how there is a little $\theta$ under the $p$ function, $\mu$ function and the $\sum$ function? That represents that the value of those functions can be represented be the parameters of neural network.

## Loss Function
Now, let's define loss function. The loss function used here is defiend based on the variational bound on negative log likelihood.

Given a negative log likelihood $\mathbb{E} [-\log p_\theta(X_0)]$, the paper takes variational bound on that likelihood via the following equation and defines the Loss function $L$

$
\mathbb{E}[-\log p_{\theta}(X_0)] \leq \mathbb{E}_q[-\log \frac{p_{\theta}(X_{0:T})}{q(X_{1:T}|X_0)}] = \mathbb{E}_q[-\log p(X_T) - \sum_{t\geq1} \log \frac{p_{\theta}(X_{t-1}|X_t)}{q(X_t|X_{t-1})}] =: L
$

// Some explanation of that loss function

// rewriting loss function in terms of mu and beta

Then, we can just simply rewrite it in terms of $\epsilon$, the guassian noise added to the image
$
\mathbb{E}_{x_0, \epsilon} \left[ \frac{1}{2 \sigma_t^2} \left\| \frac{1}{\sqrt{\alpha_t}} \left( x_t(x_0, \epsilon) - \frac{\beta_t}{\sqrt{1 - \alpha_t}} \epsilon \right) - \mu_0(x_t(x_0, \epsilon), t) \right\|^2 \right]
$

// what is epsilon? How is it reparameterized into epsilon?

## Summerizing the mathematical concepts
_"To summarize, we can train the reverse process mean function approximator $\mu_\theta$ to predict $\tilde{\mu}_\theta$, or by modifying its parameterization, we can train it to predict $\epsilon$"_ [1].

## The NN comes in

## Insights
Diffusion Model is used every where.
Following papers, recent papers

## Contact Me:
Roger Kim [[Github](https://github.com/kmsrogerkim)]

e-mail: <minseungkim1017@gmail.com> 
